{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "data_root = './data/initial'\n",
    "data_generated = './data/generated'\n",
    "train_dir = './data/train'\n",
    "val_dir = './data/val'\n",
    "test_dir = './data/initial/test'\n",
    "model_dir = './best_model'\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(os.path.join(data_generated, 'train'), train_transforms)\n",
    "val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_model(model, loss, dataloader, device):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            preds = model(inputs)\n",
    "            loss_value = loss(preds, labels)\n",
    "            preds_class = preds.argmax(dim=1)\n",
    "            running_loss += loss_value.item()\n",
    "            running_acc += (preds_class == labels.data).float().mean()\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = running_acc / len(dataloader)\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def save_model(model, val_acc, path):\n",
    "    shutil.rmtree(path, ignore_errors=True)\n",
    "    os.makedirs(path)\n",
    "    model_name = 'model_{:.3f}.torch'.format(val_acc)\n",
    "    torch.save({'state_dict': model.state_dict()}, os.path.join(path, model_name))\n",
    "\n",
    "def load_model(model_cls, path, device):\n",
    "    model = model_cls()\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, loss, optimizer, scheduler, num_epochs, eval_every_nth_batch):\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n",
    "\n",
    "        dataloader = train_dataloader\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        batch_id = 0\n",
    "        n_samples_passed = 0\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            scheduler.step()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss_value = loss(preds, labels)\n",
    "            preds_class = preds.argmax(dim=1)\n",
    "\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss_value.item()\n",
    "            running_acc += (preds_class == labels.data).float().mean()\n",
    "            batch_id += 1\n",
    "            if batch_id % eval_every_nth_batch == 0:\n",
    "                train_loss = running_loss / eval_every_nth_batch\n",
    "                train_acc = running_acc / eval_every_nth_batch\n",
    "                \n",
    "                val_loss, val_acc = validate_model(model, loss, val_dataloader, device)\n",
    "                is_best = val_acc > best_val_acc\n",
    "                if is_best:\n",
    "                    save_model(model, val_acc, model_dir)\n",
    "                    best_val_acc = val_acc\n",
    "                    val_loss_msg = 'Valid loss: {:.4f} Valid acc: {:.4f} ***'.format(val_loss, val_acc)\n",
    "                    val_loss_msg += ' CURRENTLY THE BEST!'\n",
    "                    print('Train loss: {:.4f} Train acc: {:.4f}'.format(train_loss, train_acc), flush=True)\n",
    "                    print(val_loss_msg, flush=True)\n",
    "                \n",
    "                running_loss = 0\n",
    "                running_acc = 0\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.__head = models.resnet18(pretrained=True)\n",
    "        for param in self.__head.parameters():\n",
    "            param.requires_grad = False\n",
    "        #for param in self.__head.layer4[1].parameters():\n",
    "        #    param.requires_grad = True\n",
    "        #self.__head.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "        \n",
    "        self.__bottom = torch.nn.Sequential()\n",
    "        #self.__bottom.add_module(\"linear_1_dropout\", torch.nn.Dropout(p=0.1))\n",
    "        #self.__bottom.add_module(\"linear_1\", torch.nn.Linear(self.__head.fc.in_features, 2))\n",
    "        self.__bottom.add_module(\"linear_1\", torch.nn.Linear(self.__head.fc.in_features, 256))\n",
    "        self.__bottom.add_module(\"linear_1_dropout\", torch.nn.Dropout(p=0.3))\n",
    "        self.__bottom.add_module(\"relu_1\", torch.nn.ReLU(inplace=True))\n",
    "        self.__bottom.add_module(\"linear_2\", torch.nn.Linear(256, 2))\n",
    "        \n",
    "        self.__head.fc = self.__bottom\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.__head.forward(x)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100500, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3bbc3a3e4b4ecbb3e437cf81c352c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7348 Train acc: 0.5000\n",
      "Valid loss: 0.7579 Valid acc: 0.5781 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.6543 Train acc: 0.5625\n",
      "Valid loss: 0.6530 Valid acc: 0.6589 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.4396 Train acc: 0.8125\n",
      "Valid loss: 0.5781 Valid acc: 0.7409 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.4501 Train acc: 0.8594\n",
      "Valid loss: 0.5685 Valid acc: 0.7656 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.5063 Train acc: 0.7812\n",
      "Valid loss: 0.5521 Valid acc: 0.7734 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.2362 Train acc: 0.9219\n",
      "Valid loss: 0.4977 Valid acc: 0.8008 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.3435 Train acc: 0.8906\n",
      "Valid loss: 0.4982 Valid acc: 0.8320 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.0111 Train acc: 1.0000\n",
      "Valid loss: 0.6157 Valid acc: 0.8359 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.0728 Train acc: 0.9688\n",
      "Valid loss: 0.5995 Valid acc: 0.8398 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.0012 Train acc: 1.0000\n",
      "Valid loss: 0.6642 Valid acc: 0.8555 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.0008 Train acc: 1.0000\n",
      "Valid loss: 0.6932 Valid acc: 0.8581 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.0001 Train acc: 1.0000\n",
      "Valid loss: 0.7498 Valid acc: 0.8607 *** CURRENTLY THE BEST!\n",
      "Train loss: 0.0007 Train acc: 1.0000\n",
      "Valid loss: 0.7591 Valid acc: 0.8685 *** CURRENTLY THE BEST!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "eval_every_nth_batch = 1\n",
    "n_workers = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=n_workers)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=n_workers)\n",
    "\n",
    "train_model(model, loss, optimizer, scheduler, num_epochs=1, eval_every_nth_batch=eval_every_nth_batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(Model, './resnet18_0.873.torch', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_threshold(model, val_transforms, val_dir, device):\n",
    "    model.eval()\n",
    "    best_threshold = 0.0\n",
    "    best_acc = 0\n",
    "    for threshold in tqdm(np.linspace(0, 1, 200)):\n",
    "        val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n",
    "        dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "        predictions = []\n",
    "        answers = []\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            answers.append(labels.data.numpy())\n",
    "            with torch.set_grad_enabled(False):\n",
    "                preds = model(inputs)\n",
    "            predictions.append(torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        answers = np.concatenate(answers)\n",
    "        labels = np.zeros(answers.size)\n",
    "        labels[predictions > threshold] = 1\n",
    "        accuracy = (answers == labels).mean()\n",
    "        if accuracy > best_acc:\n",
    "            print('acc: {}, threshold: {}'.format(accuracy, threshold))\n",
    "            best_acc = accuracy\n",
    "            best_threshold = threshold\n",
    "    return best_acc, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b52876c888e4a8aa9bac56e8c0d12e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5, threshold: 0.0\n",
      "acc: 0.75, threshold: 0.005025125628140704\n",
      "acc: 0.7638888888888888, threshold: 0.010050251256281407\n",
      "acc: 0.7824074074074074, threshold: 0.01507537688442211\n",
      "acc: 0.7916666666666666, threshold: 0.020100502512562814\n",
      "acc: 0.7962962962962963, threshold: 0.02512562814070352\n",
      "acc: 0.8009259259259259, threshold: 0.03015075376884422\n",
      "acc: 0.8055555555555556, threshold: 0.035175879396984924\n",
      "acc: 0.8240740740740741, threshold: 0.04522613065326633\n",
      "acc: 0.8333333333333334, threshold: 0.05025125628140704\n",
      "acc: 0.8379629629629629, threshold: 0.05527638190954774\n",
      "acc: 0.8425925925925926, threshold: 0.06532663316582915\n",
      "acc: 0.8472222222222222, threshold: 0.09045226130653267\n",
      "acc: 0.8518518518518519, threshold: 0.24623115577889448\n",
      "acc: 0.8564814814814815, threshold: 0.2562814070351759\n",
      "acc: 0.8611111111111112, threshold: 0.2914572864321608\n",
      "acc: 0.8657407407407407, threshold: 0.37185929648241206\n",
      "acc: 0.8703703703703703, threshold: 0.4020100502512563\n",
      "acc: 0.875, threshold: 0.4824120603015075\n",
      "0.875 0.4824120603015075\n"
     ]
    }
   ],
   "source": [
    "acc, threshold = find_threshold(model, val_transforms, val_dir, device)\n",
    "print(acc, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        path = os.path.basename(self.imgs[index][0])\n",
    "        path = path.split()[0]\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "    \n",
    "test_dataset = ImageFolderWithPaths(test_dir, val_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055db8b9f5d64c93bab82ec1842e46b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "test_img_paths = []\n",
    "for inputs, labels, paths in tqdm(test_dataloader):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        preds = model(inputs)\n",
    "    test_predictions.append(\n",
    "        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n",
    "    test_img_paths.extend(paths)\n",
    "    \n",
    "test_predictions = np.concatenate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000</th>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001</th>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002</th>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003</th>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004</th>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005</th>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "id         \n",
       "0000  dirty\n",
       "0001  dirty\n",
       "0002  dirty\n",
       "0003  dirty\n",
       "0004  dirty\n",
       "0005  dirty"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned')\n",
    "submission_df['id'] = submission_df['id'].str.replace('test/unknown/', '')\n",
    "submission_df['id'] = submission_df['id'].str.replace('.jpg', '')\n",
    "submission_df.set_index('id', inplace=True)\n",
    "submission_df.head(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 20/20 [00:00<00:00, 162.61it/s]\n",
      "100%|███████████████████████████████████████| 20/20 [00:00<00:00, 172.42it/s]\n"
     ]
    }
   ],
   "source": [
    "class_names = ['cleaned', 'dirty']\n",
    "\n",
    "for dir_name in [train_dir, val_dir]:\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
    "\n",
    "for class_name in class_names:\n",
    "    source_dir = os.path.join(data_root, 'train', class_name)\n",
    "    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n",
    "        if i % 6 != 0:\n",
    "            dest_dir = os.path.join(train_dir, class_name) \n",
    "        else:\n",
    "            dest_dir = os.path.join(val_dir, class_name)\n",
    "        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def prepare_dirs(root_path):\n",
    "    shutil.rmtree(root_path, ignore_errors=True)\n",
    "    os.makedirs(os.path.join(root_path, 'dirty'))\n",
    "    os.makedirs(os.path.join(root_path, 'clean'))\n",
    "\n",
    "def make_dataset(directory):\n",
    "    ts = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.3, hue=0.25),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(180.0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15))\n",
    "    ])\n",
    "    return torchvision.datasets.ImageFolder(directory, ts)\n",
    "\n",
    "def make_train_dataset():\n",
    "    return make_dataset(train_dir)\n",
    "\n",
    "def make_val_dataset():\n",
    "    return make_dataset(val_dir)\n",
    "\n",
    "def generate_images(n_amount, dataset_factory, dataset_type, root_path):\n",
    "    path = os.path.join(root_path, dataset_type)\n",
    "    prepare_dirs(path)\n",
    "    name_pattern = '{{:0{}d}}.png'.format(len(str(n_amount)))\n",
    "    class_path = {0: 'clean', 1: 'dirty'}\n",
    "    image_id = 0\n",
    "    while True:\n",
    "        dataset = dataset_factory()\n",
    "        data_iter = iter(dataset)\n",
    "        for image, class_id in data_iter:\n",
    "            if image_id >= n_amount:\n",
    "                return\n",
    "            image_name = name_pattern.format(image_id)\n",
    "            image, class_id = next(data_iter)\n",
    "            image.save(os.path.join(path, class_path[class_id], image_name), 'PNG')\n",
    "            image_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_gen_images_train = 262144\n",
    "n_gen_images_val = 4096\n",
    "\n",
    "generate_images(n_gen_images_train, make_train_dataset, 'train', data_generated)\n",
    "#generate_images(n_gen_images_val, make_val_dataset, 'val', data_generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
